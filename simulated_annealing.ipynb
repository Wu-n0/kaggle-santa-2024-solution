{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import random\n",
    "from math import exp\n",
    "from collections import Counter\n",
    "from typing import List, Optional, Union\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "PAD_TOKEN_LABEL_ID = torch.nn.CrossEntropyLoss().ignore_index\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def score(\n",
    "    solution: pd.DataFrame,\n",
    "    submission: pd.DataFrame,\n",
    "    row_id_column_name: str,\n",
    "    model_path: str = 'gemma-2-transformers-gemma-2-9b-v2',\n",
    "    load_in_8bit: bool = True,\n",
    "    clear_mem: bool = False,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculates the mean perplexity of submitted text permutations compared to an original text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    solution : DataFrame\n",
    "        DataFrame containing the original text in a column named 'text'.\n",
    "        Includes a row ID column specified by `row_id_column_name`.\n",
    "\n",
    "    submission : DataFrame\n",
    "        DataFrame containing the permuted text in a column named 'text'.\n",
    "        Must have the same row IDs as the solution.\n",
    "        Includes a row ID column specified by `row_id_column_name`.\n",
    "\n",
    "    row_id_column_name : str\n",
    "        Name of the column containing row IDs.\n",
    "        Ensures aligned comparison between solution and submission.\n",
    "\n",
    "    model_path : str\n",
    "        Path to the serialized LLM.\n",
    "\n",
    "    clear_mem : bool\n",
    "        Clear GPU memory after scoring by clearing the CUDA cache.\n",
    "        Useful for testing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The mean perplexity score. Lower is better.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ParticipantVisibleError\n",
    "        If the submission format is invalid or submitted strings are not valid permutations.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n",
    "    >>> solution = pd.DataFrame({\n",
    "    ...     'id': [0, 1],\n",
    "    ...     'text': [\"this is a normal english sentence\", \"the quick brown fox jumps over the lazy dog\"]\n",
    "    ... })\n",
    "    >>> submission = pd.DataFrame({\n",
    "    ...     'id': [0, 1],\n",
    "    ...     'text': [\"sentence english normal a is this\", \"lazy the over jumps fox brown quick the dog\"]\n",
    "    ... })\n",
    "    >>> score(solution, submission, 'id', model_path=model_path, clear_mem=True) > 0\n",
    "    True\n",
    "    \"\"\"\n",
    "    # Check that each submitted string is a permutation of the solution string\n",
    "    sol_counts = solution.loc[:, \"text\"].str.split().apply(Counter)\n",
    "    sub_counts = submission.loc[:, \"text\"].str.split().apply(Counter)\n",
    "    invalid_mask = sol_counts != sub_counts\n",
    "    if invalid_mask.any():\n",
    "        raise ParticipantVisibleError(\n",
    "            \"At least one submitted string is not a valid permutation of the solution string.\"\n",
    "        )\n",
    "\n",
    "    # Calculate perplexity for the submitted strings\n",
    "    sub_strings = [\" \".join(s.split()) for s in submission[\"text\"].tolist()]  # Split and rejoin to normalize whitespace\n",
    "    scorer = PerplexityCalculator(\n",
    "        model_path=model_path,\n",
    "        load_in_8bit=load_in_8bit,\n",
    "    )  # Initialize the perplexity calculator with a pre-trained model\n",
    "    perplexities = scorer.get_perplexity(sub_strings)  # Calculate perplexity for each submitted string\n",
    "\n",
    "    if clear_mem:\n",
    "        # Just move on if it fails. Not essential if we have the score.\n",
    "        try:\n",
    "            scorer.clear_gpu_memory()\n",
    "        except:\n",
    "            print(\"GPU memory clearing failed.\")\n",
    "\n",
    "    return float(np.mean(perplexities))\n",
    "\n",
    "\n",
    "class PerplexityCalculator:\n",
    "    \"\"\"\n",
    "    Calculates perplexity of text using a pre-trained language model.\n",
    "\n",
    "    Adapted from https://github.com/asahi417/lmppl/blob/main/lmppl/ppl_recurrent_lm.py\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_path : str\n",
    "        Path to the pre-trained language model\n",
    "\n",
    "    load_in_8bit : bool, default=False\n",
    "        Use 8-bit quantization for the model. Requires CUDA.\n",
    "\n",
    "    device_map : str, default=\"auto\"\n",
    "        Device mapping for the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        load_in_8bit: bool = False,\n",
    "        device_map: str = \"auto\",\n",
    "    ):\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path, padding_side=\"right\")\n",
    "        # Configure model loading based on quantization setting and device availability\n",
    "        if load_in_8bit:\n",
    "            if DEVICE.type != \"cuda\":\n",
    "                raise ValueError(\"8-bit quantization requires CUDA device\")\n",
    "\n",
    "            # quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n",
    "            # quantization_config = transformers.BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "            quantization_config = transformers.BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"fp4\",  # fp4 nf4\n",
    "                bnb_4bit_use_double_quant=False,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "            )\n",
    "\n",
    "            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                quantization_config=quantization_config,\n",
    "                device_map=device_map,\n",
    "            )\n",
    "        else:\n",
    "            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                torch_dtype=torch.float16 if DEVICE.type == \"cuda\" else torch.float32,\n",
    "                device_map=device_map,\n",
    "            )\n",
    "\n",
    "        self.loss_fct = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "        self.model.eval()\n",
    "        # if not load_in_8bit:\n",
    "        #    self.model.to(DEVICE)  # Explicitly move the model to the device\n",
    "\n",
    "    def get_perplexity(self, input_texts: Union[str, List[str]], batch_size: 32) -> Union[float, List[float]]:\n",
    "        \"\"\"\n",
    "        Calculates the perplexity of given texts.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_texts : str or list of str\n",
    "            A single string or a list of strings.\n",
    "\n",
    "        batch_size : int, default=None\n",
    "            Batch size for processing. Defaults to the number of input texts.\n",
    "\n",
    "        verbose : bool, default=False\n",
    "            Display progress bar.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float or list of float\n",
    "            A single perplexity value if input is a single string,\n",
    "            or a list of perplexity values if input is a list of strings.\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> import pandas as pd\n",
    "        >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n",
    "        >>> scorer = PerplexityCalculator(model_path=model_path)\n",
    "\n",
    "        >>> submission = pd.DataFrame({\n",
    "        ...     'id': [0, 1, 2],\n",
    "        ...     'text': [\"this is a normal english sentence\", \"thsi is a slihgtly misspelled zr4g sentense\", \"the quick brown fox jumps over the lazy dog\"]\n",
    "        ... })\n",
    "        >>> perplexities = scorer.get_perplexity(submission[\"text\"].tolist())\n",
    "        >>> perplexities[0] < perplexities[1]\n",
    "        True\n",
    "        >>> perplexities[2] < perplexities[0]\n",
    "        True\n",
    "\n",
    "        >>> perplexities = scorer.get_perplexity([\"this is a sentence\", \"another sentence\"])\n",
    "        >>> all(p > 0 for p in perplexities)\n",
    "        True\n",
    "\n",
    "        >>> scorer.clear_gpu_memory()\n",
    "        \"\"\"\n",
    "        single_input = isinstance(input_texts, str)\n",
    "        input_texts = [input_texts] if single_input else input_texts\n",
    "\n",
    "        loss_list = []\n",
    "\n",
    "        batches = len(input_texts) // batch_size + (len(input_texts) % batch_size != 0)\n",
    "        for j in range(batches):\n",
    "\n",
    "            a = j * batch_size\n",
    "            b = (j + 1) * batch_size\n",
    "            input_batch = input_texts[a:b]\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # Explicitly add sequence boundary tokens to the text\n",
    "                text_with_special = [\n",
    "                    f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\" for text in input_batch\n",
    "                ]\n",
    "\n",
    "                # Tokenize\n",
    "                model_inputs = self.tokenizer(\n",
    "                    text_with_special, return_tensors=\"pt\", add_special_tokens=False, padding=True\n",
    "                )\n",
    "\n",
    "                if \"token_type_ids\" in model_inputs:\n",
    "                    model_inputs.pop(\"token_type_ids\")\n",
    "\n",
    "                model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n",
    "\n",
    "                # Get model output\n",
    "                output = self.model(**model_inputs, use_cache=False)\n",
    "                logits = output[\"logits\"]\n",
    "\n",
    "                label = model_inputs[\"input_ids\"]\n",
    "                label[label == self.tokenizer.pad_token_id] = PAD_TOKEN_LABEL_ID\n",
    "\n",
    "                # Shift logits and labels for calculating loss\n",
    "                shift_logits = logits[..., :-1, :].contiguous()  # Drop last prediction\n",
    "                shift_labels = label[..., 1:].contiguous()  # Drop first input\n",
    "\n",
    "                # Calculate token-wise loss\n",
    "                loss = self.loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "                loss = loss.view(len(logits), -1)\n",
    "                valid_length = (shift_labels != PAD_TOKEN_LABEL_ID).sum(dim=-1)\n",
    "                loss = torch.sum(loss, -1) / valid_length\n",
    "\n",
    "                loss_list += loss.cpu().tolist()\n",
    "\n",
    "        ppl = [math.exp(i) for i in loss_list]\n",
    "\n",
    "        return ppl[0] if single_input else ppl\n",
    "\n",
    "    def clear_gpu_memory(self) -> None:\n",
    "        \"\"\"Clears GPU memory by deleting references and emptying caches.\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return\n",
    "\n",
    "        # Delete model and tokenizer if they exist\n",
    "        if hasattr(self, \"model\"):\n",
    "            del self.model\n",
    "        if hasattr(self, \"tokenizer\"):\n",
    "            del self.tokenizer\n",
    "\n",
    "        # Run garbage collection\n",
    "        gc.collect()\n",
    "\n",
    "        # Clear CUDA cache and reset memory stats\n",
    "        with DEVICE:\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "scorer = PerplexityCalculator('gemma-2-transformers-gemma-2-9b-v2')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    \"\"\"Take a time in seconds and return a string hh:mm:ss.\"\"\"\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimulatedAnnealing:\n",
    "    def __init__(self, Tmax, Tmin, nsteps, nsteps_per_T, log_freq, random_state, cooling, k, batch_size):\n",
    "        self.Tmax = Tmax\n",
    "        self.Tmin = Tmin\n",
    "        self.nsteps = nsteps\n",
    "        self.nsteps_per_T = nsteps_per_T\n",
    "        self.log_freq = log_freq\n",
    "        self.cooling = cooling\n",
    "        self.k = k\n",
    "        self.batch_size = batch_size\n",
    "        random.seed(random_state)\n",
    "\n",
    "    def _generate_neighbor(self, solution):\n",
    "        r = random.choice(range(3))\n",
    "        \n",
    "        if r == 0:\n",
    "            # Original method 1: Swap two random words\n",
    "            neighbor = solution.copy()\n",
    "            i, j = random.sample(range(len(neighbor)), 2)\n",
    "            neighbor[i], neighbor[j] = neighbor[j], neighbor[i]\n",
    "            return neighbor\n",
    "            \n",
    "        elif r == 1:\n",
    "            # Original method 2: Move a single word to a new position\n",
    "            shift = solution.copy()\n",
    "            extract, insert = random.sample(range(len(shift) - 1), 2)\n",
    "            shift_words = shift[extract : extract + 1]\n",
    "            shift = shift[:extract] + shift[extract + 1 :]\n",
    "            shift = shift[:insert] + shift_words + shift[insert:]\n",
    "            return shift\n",
    "            \n",
    "        elif r == 2:               \n",
    "            # Choose a starting point for the adjacent words (ensuring we have room for 2)\n",
    "            shift = solution.copy()\n",
    "            extract = random.randint(0, len(shift) - 2)\n",
    "            \n",
    "            # Extract the two adjacent words\n",
    "            shift_words = shift[extract : extract + 2]\n",
    "            \n",
    "            # Remove them from the original position\n",
    "            shift = shift[:extract] + shift[extract + 2 :]\n",
    "            \n",
    "            # Choose an insertion point in the remaining sequence\n",
    "            insert = random.randint(0, len(shift))\n",
    "            \n",
    "            # Insert the two words at the new position\n",
    "            shift = shift[:insert] + shift_words + shift[insert:]\n",
    "            return shift\n",
    "\n",
    "    def _acceptance_probability(self, current_energy, new_energy, temperature):\n",
    "        \"\"\"\n",
    "        Calculate the probability of accepting a new solution.\n",
    "        \"\"\"\n",
    "        if new_energy < current_energy:\n",
    "            return 1.0\n",
    "        return math.exp(self.k * (current_energy - new_energy) / temperature)\n",
    "\n",
    "    def solve(self, text):\n",
    "        t0 = time.time()  # Measure starting time\n",
    "\n",
    "        current_solution = text.split()\n",
    "        current_energy = scorer.get_perplexity(\" \".join(current_solution), batch_size=self.batch_size)\n",
    "\n",
    "        best_solution = current_solution.copy()\n",
    "        best_energy = current_energy\n",
    "\n",
    "        temperature = self.Tmax\n",
    "        Tfactor = -math.log(self.Tmax / self.Tmin)  # for exponential cooling\n",
    "\n",
    "        temperatures = [temperature]\n",
    "        log_energies = [current_energy]\n",
    "        \n",
    "        # Counter for tracking iterations without improvement\n",
    "        no_improvement_count = 0\n",
    "        \n",
    "        for step in range(self.nsteps):\n",
    "            print('Current Step:', step)\n",
    "            accept = 0\n",
    "\n",
    "            for step1 in tqdm(range(self.nsteps_per_T), desc=\"Simulated Annealing Progress\"):\n",
    "                # Generate neighbor\n",
    "                new_solution = self._generate_neighbor(current_solution)\n",
    "                new_energy = scorer.get_perplexity(\" \".join(new_solution), batch_size=self.batch_size)\n",
    "\n",
    "                # Calculate acceptance probability\n",
    "                acceptance = self._acceptance_probability(current_energy, new_energy, temperature)\n",
    "\n",
    "                # Update current solution\n",
    "                if acceptance > random.random():\n",
    "                    current_solution = new_solution\n",
    "                    current_energy = new_energy\n",
    "                    accept += 1\n",
    "\n",
    "                # Update best solution\n",
    "                if new_energy < best_energy:\n",
    "                    best_solution = new_solution.copy()\n",
    "                    best_energy = new_energy\n",
    "                    print(f\"\\nNew best score: {best_energy:8.3f}\")\n",
    "                    print(\"New text:\", \" \".join(best_solution), \"\\n\", flush=True)\n",
    "                    no_improvement_count = 0  # Reset counter when we find an improvement\n",
    "                else:\n",
    "                    no_improvement_count += 1  # Increment counter when no improvement\n",
    "                \n",
    "                # Apply \"kick\" if stuck in local optima (10,000 iterations without improvement)\n",
    "                if no_improvement_count >= 10000:\n",
    "                    # Increase temperature to a value between current temp and (Tmax/2 + current_temp)/2\n",
    "                    kick_temp = temperature + random.uniform(0, (self.Tmax/2 - temperature))\n",
    "                    print(f\"\\nKICK APPLIED! No improvement for {no_improvement_count} iterations.\")\n",
    "                    print(f\"Increasing temperature from {temperature:.4f} to {kick_temp:.4f}\\n\")\n",
    "                    temperature = kick_temp\n",
    "                    no_improvement_count = 0  # Reset counter after kick\n",
    "\n",
    "                # Log progress\n",
    "                log_energies.append(current_energy)\n",
    "                temperatures.append(temperature)\n",
    "\n",
    "                t1 = format_time(time.time() - t0)\n",
    "\n",
    "                if step1 % self.log_freq == 0 or step1 == (self.nsteps_per_T - 1):\n",
    "                    print(\n",
    "                        f\"T: {temperature:8.3f}  Step: {step1:6}  Acceptance Rate: {accept/(step1+1):7.4f}  \"\n",
    "                        f\"Score: {current_energy:8.3f}  Best Score: {best_energy:8.3f}  \"\n",
    "                        f\"Elapsed Time: {t1}  No Improvement: {no_improvement_count}\",\n",
    "                        flush=True,\n",
    "                    )\n",
    "\n",
    "            print('Current Text:', \" \".join(current_solution))\n",
    "\n",
    "            # Lower the temperature according to the cooling schedule\n",
    "            if self.cooling == \"linear\":\n",
    "                temperature -= (self.Tmax - self.Tmin) / self.nsteps\n",
    "            elif self.cooling == \"exponential\":\n",
    "                temperature = self.Tmax * math.exp(Tfactor * (step + 1) / self.nsteps)\n",
    "            elif self.cooling == \"logarithmic\":\n",
    "                temperature = self.Tmax / math.log10(step + 10)\n",
    "\n",
    "            if best_energy < 68.:\n",
    "                print(\"Stop! Target value is achieved.\")\n",
    "                break\n",
    "\n",
    "        return \" \".join(best_solution), best_energy, log_energies, temperatures, \" \".join(current_solution), current_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_params = {\n",
    "    'Tmax': 0.7,\n",
    "    'Tmin': 0.1,\n",
    "    'nsteps': 30,\n",
    "    'nsteps_per_T': 1000,\n",
    "    'log_freq': 250,\n",
    "    'random_state': 321,\n",
    "    'cooling': 'linear',\n",
    "    'k': 1.,\n",
    "    'batch_size': 4\n",
    "}\n",
    "\n",
    "optimizer = SimulatedAnnealing(**sa_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'of and to in the as you that it we with from have not night season eggnog fruitcake toy doll game puzzle snowglobe cookie milk chocolate peppermint candy candle fireplace wreath poinsettia greeting card wrapping paper bow wish dream believe wonder hope star angel joy peace merry hohoho kaggle workshop'\n",
    "best_solution, best_score, log_scores, log_ts, final_solution, final_score = optimizer.solve(text)\n",
    "print(\"Best Score:\", best_score)\n",
    "print(\"Best Solution:\", best_solution)\n",
    "print('Final Score:', final_score)\n",
    "print('Final Solution:', final_solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(log_scores, label=\"SA Score Progression\")\n",
    "plt.title(\"Simulated Annealing Progression\")\n",
    "plt.xlabel(\"SA Iteration\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final SA Score: {final_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
